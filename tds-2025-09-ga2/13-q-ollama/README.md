## 1. Problem Description

The task is to run a local instance of the Ollama large language model server and expose it to the public internet using `ngrok`. The setup must be configured to allow Cross-Origin Resource Sharing (CORS) and to inject a custom `X-Email` HTTP header into all responses.

## 2. Understanding the Requirements

* **LLM Server**: Ollama (`http://localhost:11434`).
* **CORS**: Ollama must be configured to accept requests from any origin (` OLLAMA_ORIGINS="*" `).
* **Tunneling**: `ngrok` must be used to create a public HTTPS URL for the local Ollama server.
* **Header Injection**: `ngrok` must add the header `X-Email: 23f3004197@ds.study.iitm.ac.in` to every response.
* **Submission**: The public HTTPS forwarding URL generated by `ngrok`.

## 3. Step-by-Step Solution

#### Step 1: Install and Prepare Tools

1. **Install Ollama**: Download and install Ollama from the [official website](https://ollama.com/). After installation, pull a model to ensure it's working:

    ```bash
    ollama pull llama3
    ```

2. **Install ngrok**: Download ngrok from the [official website](https://www.google.com/search?q=https://ngrok.com/download). Unzip it and place the executable in your system's PATH.
3. **Configure ngrok**: Sign up for a free ngrok account to get an authtoken. Configure the ngrok agent with your token (this is a one-time step):

    ```bash
    ngrok config add-authtoken YOUR_AUTHTOKEN
    ```

#### Step 2: Run Ollama with CORS Enabled

1. Open a **new terminal window** (Terminal 1).
2. Set the `OLLAMA_ORIGINS` environment variable and start the Ollama server. This command keeps the server running until you stop it.

    ```bash
    export OLLAMA_ORIGINS="*"
    ollama serve
    ```

    You should see output indicating that the server is listening on `127.0.0.1:11434`. Keep this terminal running.

    > If it says `Error: listen tcp 127.0.0.1:11434: bind: address already in use`, it means another instance of Ollama is already running. You need to stop that instance by doing `systemctl stop ollama` and again run `ollama serve` and keep the terminal open.

#### Step 3: Run ngrok to Expose Ollama

1. Open a **second terminal window** (Terminal 2).
2. Run the `ngrok` command to create a tunnel to your local Ollama port (11434). This command includes flags to add the required CORS and custom email headers.

    ```bash
    ngrok http 11434 \
    --response-header-add "X-Email: 23f3004197@ds.study.iitm.ac.in" \
    --response-header-add "Access-Control-Expose-Headers: *" \
    --response-header-add "Access-Control-Allow-Headers: Ngrok-skip-browser-warning" \
    --host-header=localhost
    ```

   #### Explanation of ngrok options

    1. **`ngrok http 11434`**

    * Exposes your local Ollama server on port `11434` to a public URL.
    * Anyone with the ngrok URL can access your server (CORS rules still apply).

    2. **`--response-header-add "X-Email: ..."`**

    * Adds a custom header `X-Email` to all responses.
    * Useful for debugging, tracking, or verifying requests from the frontend.

    3. **`--response-header-add "Access-Control-Expose-Headers: *"`**

    * Allows JavaScript (e.g., `fetch`) to read **all headers** in cross-origin responses.
    * Without this, only default headers like `Content-Type` are accessible.

    4. **`--response-header-add "Access-Control-Allow-Headers: Ngrok-skip-browser-warning"`**

    * Lets browsers send the custom `Ngrok-skip-browser-warning` header in requests.
    * Needed to bypass ngrokâ€™s browser warning page.
    * Prevents CORS errors in your web app.

    5. **`--host-header=localhost`**

    * Rewrites the `Host` header sent to your local server as `localhost`.
    * Ensures Ollama sees requests as coming from a local origin.
    * Avoids `403 Forbidden` errors when requests come through ngrok.

3. `ngrok` will start and display a UI in your terminal. Look for the `Forwarding` URL that starts with `https://`. It will look something like `https://<random-string>.ngrok-free.app`.
4. Copy this HTTPS URL. This is the answer you will submit.

## 4. Code / Configuration Files

This task does not require writing any code files, only running commands in the terminal.

**Command for Terminal 1 (Ollama):**

```bash
export OLLAMA_ORIGINS="*"
ollama serve
```

**Command for Terminal 2 (ngrok):**

```bash
ngrok http 11434 \
--response-header-add "X-Email: 23f3004197@ds.study.iitm.ac.in" \
--response-header-add "Access-Control-Expose-Headers: *" \
--response-header-add "Access-Control-Allow-Headers: Ngrok-skip-browser-warning" \
--host-header=localhost
```

## 5. How to Run and Verify

Before submitting, you can verify that your setup is correct using `curl`.

1. In a **third terminal window**, run the following command, replacing `<YOUR_NGROK_URL>` with the URL you copied from `ngrok`. The `-I` flag tells curl to fetch only the headers.

    ```bash
    curl -I <YOUR_NGROK_URL>/api/version --header "ngrok-skip-browser-warning: true"
    ```

2. Inspect the output. You should see the following headers, confirming your setup is correct:
    * `HTTP/2 200` (indicating a successful connection)
    * `access-control-allow-headers: Ngrok-skip-browser-warning`
    * `access-control-expose-headers: *`
    * `x-email: 23f3004197@ds.study.iitm.ac.in`

Once verified, submit the `https://...` URL from `ngrok` to the exam platform. Keep both the Ollama and ngrok terminals running.
